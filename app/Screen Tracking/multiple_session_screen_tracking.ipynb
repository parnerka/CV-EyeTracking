{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screen Tracking Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Importing Required Libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kb/108ht9mj2xdgthy9ngx9dx6r0000gn/T/ipykernel_90636/2507435717.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from pupil_apriltags import Detector\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.spatial.distance import pdist\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import csv\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Initializing the constructor corresponding to the April Tag detection library</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "csv_data = []\n",
    "\n",
    "at_detector_36h11 = Detector(\n",
    "    families=\"tag36h11\",\n",
    "    nthreads=4,\n",
    "    quad_decimate=1.0,\n",
    "    quad_sigma=0.0,\n",
    "    refine_edges=1,\n",
    "    decode_sharpening=0.25,\n",
    "    debug=0\n",
    ")\n",
    "\n",
    "# Output Video Frame Rate - Editable\n",
    "output_frame_rate = 24\n",
    "\n",
    "screen_coordinate_columns = ['BL', 'BR', 'TR', 'TL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the video location and the tobii data location\n",
    "\n",
    "Example:<br>\n",
    "video_loc = \"C:\\\\Users\\\\ARL\\\\GroundingDINO\\\\app\\\\app_data_demo\\\\demo_video.mp4\"<br>\n",
    "tobii_data = pd.read_csv(\"C:\\\\Users\\\\ARL\\\\GroundingDINO\\\\app\\\\app_data_demo\\\\app_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple sessions\n",
    "\n",
    "# Please input the gaze data csv file and the video file for each session in the format shown below.\n",
    "# The key is the csv file (left) and the value is the video file (right).\n",
    "\n",
    "session_dict = {'data/app_data.csv': 'data/demo_video.mp4',\n",
    "                'data/app_data_2.csv': 'data/demo_video_2.mp4',\n",
    "                'data/app_data_3.csv': 'data/demo_video_3.mp4'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explanation of what the next 2 code cells do : </b>These code cells are used for the following tasks:\n",
    "\n",
    "1. The `find_screen_containing_point` function takes in the screen results (centres, corners, etc) and the gaze point to check which screen is currently being looked at. It also leverages the `is_point_inside_rectangle` function as a helper function to check if the point is inside a specific bounding box or not. \n",
    "\n",
    "2. Once the current screen being looked at is found, the `plot_det_april` function along with its helper functions (`select_tightest_pack`, `calculate_total_distance`) find the 4 points which are closest to the screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def calculate_total_distance(points):\n",
    "    distances = pdist(points)\n",
    "    return np.sum(distances)\n",
    "\n",
    "def select_tightest_pack(points_list):\n",
    "    min_total_distance = float('inf')\n",
    "    tightest_pack = None\n",
    "\n",
    "    for combination in itertools.product(*points_list):\n",
    "        total_distance = calculate_total_distance(combination)\n",
    "        if total_distance < min_total_distance:\n",
    "            min_total_distance = total_distance\n",
    "            tightest_pack = combination\n",
    "\n",
    "    return tightest_pack\n",
    "\n",
    "def plot_det_april(image, results, label):\n",
    "    point_collection = []\n",
    "    for r in results:\n",
    "        (ptA, ptB, ptC, ptD) = r.corners\n",
    "        ptB = (int(ptB[0]), int(ptB[1]))\n",
    "        ptC = (int(ptC[0]), int(ptC[1]))\n",
    "        ptD = (int(ptD[0]), int(ptD[1]))\n",
    "        ptA = (int(ptA[0]), int(ptA[1]))\n",
    "\n",
    "        point_collection.append([ptB, ptC, ptD, ptA])\n",
    "\n",
    "    min_rectangle_points = select_tightest_pack(point_collection)\n",
    "    return min_rectangle_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def is_point_inside_rectangle(point, rectangle):\n",
    "    x, y = point\n",
    "    min_x = np.min(rectangle[:, 0])\n",
    "    max_x = np.max(rectangle[:, 0])\n",
    "    min_y = np.min(rectangle[:, 1])\n",
    "    max_y = np.max(rectangle[:, 1])\n",
    "\n",
    "    if min_x <= x <= max_x and min_y <= y <= max_y:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def find_screen_containing_point(screens, point):\n",
    "    for screen_index, screen_results in enumerate(screens):\n",
    "        if len(screen_results) > 0:\n",
    "            centers = np.array([detection.center for detection in screen_results])\n",
    "            min_x = np.min(centers[:, 0])\n",
    "            max_x = np.max(centers[:, 0])\n",
    "            min_y = np.min(centers[:, 1])\n",
    "            max_y = np.max(centers[:, 1])\n",
    "            bounnding_rectangle = np.array([[min_x, min_y], [max_x, min_y], [min_x, max_y], [max_x, max_y]])\n",
    "\n",
    "            if is_point_inside_rectangle(point, bounnding_rectangle):\n",
    "                return screen_index\n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The `screen_track_single_img` function is used as a main function when a frame from the video is extracted.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def screen_track_single_img(image, gaze2d, timestamp, op_ts):\n",
    "    if gaze2d is None:\n",
    "        return\n",
    "    \n",
    "    image_height, image_width, _ = image.shape\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    results_36h11 = at_detector_36h11.detect(gray)\n",
    "    available_screens = {\n",
    "        'screen1': [],\n",
    "        'screen2': [],\n",
    "        'screen3': [],\n",
    "        'screen4': []\n",
    "    }\n",
    "\n",
    "    for detection in results_36h11:\n",
    "        tag_id = detection.tag_id\n",
    "\n",
    "        if tag_id in [1, 2, 3, 4]:\n",
    "            available_screens['screen1'].append(tag_id)\n",
    "        if tag_id in [5, 6, 7, 8]:\n",
    "            available_screens['screen2'].append(tag_id)\n",
    "        if tag_id in [9, 10, 11, 12]:\n",
    "            available_screens['screen3'].append(tag_id)\n",
    "        if tag_id in [13, 14, 15, 16]:\n",
    "            available_screens['screen4'].append(tag_id)\n",
    "\n",
    "    for screen, tags in available_screens.items():\n",
    "        if len(tags) == 4:\n",
    "            available_screens[screen] = tags\n",
    "\n",
    "    screen1_results, screen2_results, screen3_results, screen4_results = [], [], [], []\n",
    "\n",
    "    for detection in results_36h11:\n",
    "        tag_id = detection.tag_id\n",
    "        if tag_id in available_screens['screen1']:\n",
    "            screen1_results.append(detection)\n",
    "        if tag_id in available_screens['screen2']:\n",
    "            screen2_results.append(detection)\n",
    "        if tag_id in available_screens['screen3']:\n",
    "            screen3_results.append(detection)\n",
    "        if tag_id in available_screens['screen4']:\n",
    "            screen4_results.append(detection)\n",
    "\n",
    "    screens = [screen1_results, screen2_results, screen3_results, screen4_results]\n",
    "    point = (gaze2d[0]*image_width, gaze2d[1]*image_height)\n",
    "    screen_index = find_screen_containing_point(screens, point)\n",
    "\n",
    "    if screen_index is not None:\n",
    "        min_rectangle_points = plot_det_april(image, screens[screen_index], f\"Screen {screen_index+1}\")\n",
    "        min_rectangle_points += (np.NaN,) * (4 - len(min_rectangle_points))\n",
    "        csv_data.append({\n",
    "            'timestamp': timestamp,\n",
    "            'Output_Video_Timestamp': op_ts,\n",
    "            'gaze2d_x': point[0],\n",
    "            'gaze2d_y': point[1],\n",
    "            'Screen': screen_index+1,\n",
    "            'BL': min_rectangle_points[0],\n",
    "            'BR': min_rectangle_points[1],\n",
    "            'TR': min_rectangle_points[2],\n",
    "            'TL': min_rectangle_points[3]\n",
    "        })\n",
    "    else:\n",
    "        csv_data.append({\n",
    "            'timestamp': timestamp,\n",
    "            'Output_Video_Timestamp': op_ts,\n",
    "            'gaze2d_x': point[0],\n",
    "            'gaze2d_y': point[1],\n",
    "            'Screen': np.NaN,\n",
    "            'BL': np.NaN,\n",
    "            'BR': np.NaN,\n",
    "            'TR': np.NaN,\n",
    "            'TL': np.NaN\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# MAIN FUNCTION\n",
    "def batch_process_screen_det(video_loc, matched_rows):\n",
    "    cap = cv2.VideoCapture(video_loc)\n",
    "    total_frames = len(matched_rows)\n",
    "    ti = 1 / output_frame_rate\n",
    "    op_ts = 0.0\n",
    "    pbar = tqdm(total=total_frames, desc='Processing Frames')\n",
    "    for _, row in matched_rows.iterrows():\n",
    "        timestamp = row['timestamp']\n",
    "        if pd.notna(timestamp):\n",
    "            frame_number = int(timestamp * cap.get(cv2.CAP_PROP_FPS))\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                gaze2d = (row['gaze2d_x'], row['gaze2d_y'])\n",
    "                screen_track_single_img(frame, gaze2d, timestamp, op_ts)\n",
    "                op_ts += ti\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The code cell below is used to visualize the screen detection results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def visualize(video_loc, csv_df, current_datetime):\n",
    "    cap = cv2.VideoCapture(video_loc)\n",
    "    output_video = cv2.VideoWriter(f\"screen_tracking_results_video_{current_datetime}.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), output_frame_rate, (1920, 1080))\n",
    "\n",
    "    current_frame = None\n",
    "\n",
    "    total_frames = len(csv_df)\n",
    "    pbar = tqdm(total=total_frames, desc='Processing Frames')\n",
    "\n",
    "    for index, row in csv_df.iterrows():\n",
    "        timestamp = row['timestamp']\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, int(timestamp*1000))\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        screen_coordinates = [row[col] for col in screen_coordinate_columns if not pd.isna(row[col])]\n",
    "        screen_coordinates = np.array(screen_coordinates, dtype=np.int32)\n",
    "        gaze_m = (int(row['gaze2d_x']), int(row['gaze2d_y'])) if not pd.isna(row['gaze2d_x']) and not pd.isna(row['gaze2d_y']) else (None, None)\n",
    "        screen_number = row['Screen']\n",
    "        if pd.isna(gaze_m[0]) or pd.isna(gaze_m[1]):\n",
    "            gaze_text = 'Gaze not found'\n",
    "        else:\n",
    "            gaze_text = 'Gaze Available'\n",
    "            cv2.circle(frame, gaze_m, 15, (255, 0, 0), -1)\n",
    "\n",
    "            if pd.isna(screen_number):\n",
    "                screen_text = 'Screen not detected'\n",
    "            else:\n",
    "                if len(screen_coordinates) < 4:\n",
    "                    if not pd.isna(screen_number):\n",
    "                        screen_text = f'Not all April tags were detected but predicted screen = {int(screen_number)}'\n",
    "                    # else:\n",
    "                    #     screen_text = 'Not all April tags were detected'\n",
    "                else:\n",
    "                    screen_text = f'Screen {int(screen_number)}'\n",
    "                    hull = cv2.convexHull(screen_coordinates, clockwise=True)\n",
    "                    cv2.drawContours(frame, [hull], -1, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.putText(frame, screen_text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, gaze_text, (20, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0 ,0), 2)\n",
    "        output_video.write(frame)\n",
    "        cv2.waitKey(50)\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    cap.release()\n",
    "    output_video.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapy run function\n",
    "def run(session_dict):\n",
    "    for data_file, video_file in session_dict.items():\n",
    "        global csv_data\n",
    "        tobii_data = pd.read_csv(data_file)\n",
    "        print('---------------------------------------------')\n",
    "        print(f\"Processing {data_file} and {video_file}\")\n",
    "        batch_process_screen_det(video_file, tobii_data)   # Processing started\n",
    "        current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        csv_df = pd.DataFrame(csv_data)\n",
    "        csv_df.to_csv(f\"screen_tracking_results_{current_datetime}.csv\", index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        print(f\"Results saved to screen_tracking_results_{current_datetime}.csv\")\n",
    "        print(f\"Visualizing for {data_file} and {video_file}...\")\n",
    "        visualize(video_file, csv_df, current_datetime)   # Visualization started\n",
    "        print(f\"Visualization completed for {data_file} and {video_file}\")\n",
    "        csv_data = []\n",
    "        print('---------------------------------------------')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Processing data/app_data.csv and data/demo_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 75/75 [00:10<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to screen_tracking_results_2024-05-19_19-58-43.csv\n",
      "Visualizing for data/app_data.csv and data/demo_video.mp4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 75/75 [00:13<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization completed for data/app_data.csv and data/demo_video.mp4\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "Processing data/app_data_2.csv and data/demo_video_2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 75/75 [00:10<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to screen_tracking_results_2024-05-19_19-59-07.csv\n",
      "Visualizing for data/app_data_2.csv and data/demo_video_2.mp4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 75/75 [00:13<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization completed for data/app_data_2.csv and data/demo_video_2.mp4\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "Processing data/app_data_3.csv and data/demo_video_3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 75/75 [00:11<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to screen_tracking_results_2024-05-19_19-59-33.csv\n",
      "Visualizing for data/app_data_3.csv and data/demo_video_3.mp4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 75/75 [00:13<00:00,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization completed for data/app_data_3.csv and data/demo_video_3.mp4\n",
      "---------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run(session_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
